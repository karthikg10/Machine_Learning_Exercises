# -*- coding: utf-8 -*-
"""TakeHomeExam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wp1bA4gVkLu-gglmGVKYlWJ6W8Sklpp0
"""

# Task-1: Step 1

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.impute import SimpleImputer

# Load training dataset
train_dataset = '/content/synthetic_dataset (1).csv'
train_df = pd.read_csv(train_dataset)

# Load validation dataset
val_dataset = '/content/synthetic_test_dataset.csv'
val_df = pd.read_csv(val_dataset)

# Replace '########' with NaN
train_df.replace('########', pd.NA, inplace=True)
val_df.replace('########', pd.NA, inplace=True)

# Convert columns to numeric
train_df = train_df.apply(pd.to_numeric, errors='coerce')
val_df = val_df.apply(pd.to_numeric, errors='coerce')

# Impute missing values with the mean of the corresponding columns for both datasets
imputer = SimpleImputer(strategy='mean')
train_df_imp = pd.DataFrame(imputer.fit_transform(train_df), columns=train_df.columns)
val_df_imp = pd.DataFrame(imputer.transform(val_df), columns=val_df.columns)

# Assuming the target variable is named 'y' and other columns are features
X_train = train_df_imp.drop('y', axis=1)
y_train = train_df_imp['y']

X_val = val_df_imp.drop('y', axis=1)
y_val = val_df_imp['y']

# Train a linear model
linear_model = LinearRegression()
linear_model.fit(X_train, y_train)

# Make predictions on training and validation data
y_train_pred = linear_model.predict(X_train)
y_val_pred = linear_model.predict(X_val)

# Calculate Mean Squared Error (MSE) for training and validation
train_error = mean_squared_error(y_train, y_train_pred)
val_error = mean_squared_error(y_val, y_val_pred)

# Print the errors
print(f"Training Error: {train_error}")
print(f"Validation Error: {val_error}")

# Step 2

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_squared_error

# Function to train a linear model with polynomial features
def train_linear(X_train, y_train, X_val, y_val, degree):
    # Create polynomial features
    poly = PolynomialFeatures(degree=degree)
    X_train_poly = poly.fit_transform(X_train)
    X_val_poly = poly.transform(X_val)

    # Train a linear model
    linear_model = LinearRegression()
    linear_model.fit(X_train_poly, y_train)

    # Make predictions on training and validation data
    y_train_pred = linear_model.predict(X_train_poly)
    y_val_pred = linear_model.predict(X_val_poly)

    # Calculate Mean Squared Error (MSE) for training and validation
    train_error = mean_squared_error(y_train, y_train_pred)
    val_error = mean_squared_error(y_val, y_val_pred)

    return train_error, val_error

# Train linear models with polynomial features of different orders
for degree in [2, 3, 4]:
    train_error, val_error = train_linear(X_train, y_train, X_val, y_val, degree)
    print(f"\nDegree-{degree} Polynomial Model:")
    print(f"Training Error: {train_error}")
    print(f"Validation Error: {val_error}")

# Step 3

import matplotlib.pyplot as plt
import numpy as np

# Initialize lists to store errors for each polynomial degree
degrees = [2, 3, 4]
train_errors = []
val_errors = []

# Train linear models with polynomial features of different orders
for degree in degrees:
    train_error, val_error = train_linear(X_train, y_train, X_val, y_val, degree)
    train_errors.append(train_error)
    val_errors.append(val_error)

# Plotting the errors
plt.figure(figsize=(10, 6))
plt.plot(degrees, train_errors, marker='o', label='Training Error')
plt.plot(degrees, val_errors, marker='o', label='Validation Error')
plt.title('Training and Validation Errors by Polynomial Degree')
plt.xlabel('Polynomial Degree')
plt.ylabel('Mean Squared Error')
plt.legend()
plt.grid(True)
plt.show()

# Task-2

import pandas as pd
from sklearn.model_selection import train_test_split

# Load the preprocessed dataset
dataset = '/content/breast_cancer_dataset_preprocessed.csv'
cancer_df = pd.read_csv(dataset)

# Display the first few rows of the dataset
print(cancer_df.head())

# Split the dataset into features (X) and the target variable (y)
X = cancer_df.drop('y', axis=1)
y = cancer_df['y']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=766)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score


# Split the dataset into features (X) and the target variable (y)
X = cancer_df.drop('y', axis=1)
y = cancer_df['y']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=766)

# Define and train different classification models
models = {
    'Logistic Regression': LogisticRegression(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC(),

}

for name, model in models.items():
    # Train the model
    model.fit(X_train, y_train)

    # Make predictions on the test set
    y_pred = model.predict(X_test)

    # Evaluate the model
    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred, pos_label='M')
    recall = recall_score(y_test, y_pred, pos_label='M')
    f1 = f1_score(y_test, y_pred, pos_label='M')

    # Print the evaluation metrics
    print(f"\n{name} Metrics:")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"F1-Score: {f1:.4f}")